# -*- coding: utf-8 -*-
"""FNDFDS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bCHyCqpQ9QEB6r54AUg3SmZ2mM-bZZc
"""

!pip install transformers datasets evaluate scikit-learn

import pandas as pd

fakenews_df = pd.read_csv('/content/Fake.csv')
truenews_df = pd.read_csv('/content/True.csv')

print(fakenews_df.columns)
print(truenews_df.columns)

print(fakenews_df.shape)
print(truenews_df.shape)

truenews_df['two_way_label'] = 0
fakenews_df['two_way_label'] = 1

truenews_df.head()

fakenews_df.head()

combined_df = pd.concat([truenews_df, fakenews_df], ignore_index=True)

randomized_df = combined_df.sample(frac=1).reset_index(drop=True)

randomized_df.head()

file_path = '/content/randomized_news_data.csv'
randomized_df.to_csv(file_path, index=False)

print(f"\n✅ DataFrame successfully saved to {file_path}")

# create trin_df, val_df, test_df from /content/randomized_news_data.csv
import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('/content/randomized_news_data.csv')

# split the df into train and test dfs
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)



print(train_df.shape)
print(test_df.shape)

train_df.head()

test_df.head()

# split train_df into train_df and val_df
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

print(train_df.shape)
print(val_df.shape)
print(test_df.shape)

print(train_df.columns)
print(val_df.columns)
print(test_df.columns)

# import pandas as pd

# train_df = pd.read_csv('/content/multimodal_train.tsv', sep='\t')
# val_df   = pd.read_csv('/content/multimodal_validate.tsv', sep='\t')
# test_df  = pd.read_csv('/content/multimodal_test_public.tsv', sep='\t')

# print(train_df.columns)  # check available columns

# print(train_df.shape)
# print(val_df.shape)
# print(test_df.shape)

# # Select the first 10,000 rows of each dataframe
# train_df_small = train_df.iloc[:10000]
# val_df_small = val_df.iloc[:2000]
# test_df_small = test_df.iloc[:2000]

# # Verify the new shapes
# print(train_df_small.shape)
# print(val_df_small.shape)
# print(test_df_small.shape)

from datasets import Dataset

train_ds = Dataset.from_pandas(train_df[['text', 'two_way_label']])
val_ds   = Dataset.from_pandas(val_df[['text', 'two_way_label']])
test_ds  = Dataset.from_pandas(test_df[['text', 'two_way_label']])

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# --- 1. Set up the device (GPU or CPU) ---
# This is the crucial part. It checks if a CUDA-enabled GPU is available.
# In Colab, with the T4 runtime selected, torch.cuda.is_available() will be True.
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"✅ Found GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("⚠️ No GPU found, using CPU. This will be very slow.")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=64)

train_ds = train_ds.map(tokenize, batched=True)
val_ds   = val_ds.map(tokenize, batched=True)
test_ds  = test_ds.map(tokenize, batched=True)

train_ds = train_ds.rename_column("two_way_label", "labels")
val_ds   = val_ds.rename_column("two_way_label", "labels")
test_ds  = test_ds.rename_column("two_way_label", "labels")

train_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# !pip install --upgrade transformers

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate

# Load BERT
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

model.to(device)
print(f"Model moved to {device}.")

# Metrics
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels),
        "f1": f1.compute(predictions=preds, references=labels)
    }

# Training Args
training_args = TrainingArguments(
    output_dir="./results",


    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

# test and validate the model
print(trainer.evaluate(val_ds))
print(trainer.evaluate(test_ds))

model.save_pretrained("./text_model")
tokenizer.save_pretrained("./text_model")

# predict
inputText = "You won't believe what this celebrity was caught doing, shocking photo inside"

# 1. Define the path to your saved, fine-tuned model
model_path = "./text_model"

# 2. Load the tokenizer and model from that directory
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# 3. Set up the device (use GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval() # Put the model in evaluation mode

# 4. Define the input text you want to classify
inputText = "You won't believe what this celebrity was caught doing, shocking photo inside"

# 5. Tokenize the text and move tensors to the correct device
inputs = tokenizer(inputText, return_tensors="pt").to(device)

# 6. Make a prediction
with torch.no_grad(): # Disable gradient calculation for faster inference
    logits = model(**inputs).logits

# 7. Get the predicted class ID (0 or 1)
predicted_class_id = logits.argmax().item()

# 8. Print the final, human-readable result
print(f"Input Text: '{inputText}'")
if predicted_class_id == 1:
    print("--> Prediction: Fake News")
else:
    print("--> Prediction: True News")

inputText = "WASHINGTON (Reuters) - Trump campaign adviser George Papadopoulos told an Australian diplomat in May 2016 that Russia had political dirt on Democratic presidential candidate Hillary Clinton, the New York Times reported on Saturday. The conversation between Papadopoulos and the diplomat, Alexander Downer, in London was a driving factor behind the FBI‚Äôs decision to open a counter-intelligence investigation of Moscow‚Äôs contacts with the Trump campaign, the Times reported. Two months after the meeting, Australian officials passed the information that came from Papadopoulos to their American counterparts when leaked Democratic emails began appearing online, according to the newspaper, which cited four current and former U.S. and foreign officials. Besides the information from the Australians, the probe by the Federal Bureau of Investigation was also propelled by intelligence from other friendly governments, including the British and Dutch, the Times said. Papadopoulos, a Chicago-based international energy lawyer, pleaded guilty on Oct. 30 to lying to FBI agents about contacts with people who claimed to have ties to top Russian officials. It was the first criminal charge alleging links between the Trump campaign and Russia. The White House has played down the former aide‚Äôs campaign role, saying it was ‚Äúextremely limited‚Äù and that any actions he took would have been on his own. The New York Times, however, reported that Papadopoulos helped set up a meeting between then-candidate Donald Trump and Egyptian President Abdel Fattah al-Sisi and edited the outline of Trump‚Äôs first major foreign policy speech in April 2016. The federal investigation, which is now being led by Special Counsel Robert Mueller, has hung over Trump‚Äôs White House since he took office almost a year ago. Some Trump allies have recently accused Mueller‚Äôs team of being biased against the Republican president. Lawyers for Papadopoulos did not immediately respond to requests by Reuters for comment. Mueller‚Äôs office declined to comment. Trump‚Äôs White House attorney, Ty Cobb, declined to comment on the New York Times report. ‚ÄúOut of respect for the special counsel and his process, we are not commenting on matters such as this,‚Äù he said in a statement. Mueller has charged four Trump associates, including Papadopoulos, in his investigation. Russia has denied interfering in the U.S. election and Trump has said there was no collusion between his campaign and Moscow. "

# 4. Define the input text you want to classify
# inputText = "Woman sentenced in case that sparked Springfield cat-eating rumours Allexis Ferrell, a US citizen, attacked a cat in a city far from a Haitian immigrant community in Ohio."
# inputText = "Azerbaijan alleges India blocked its SCO full membership bid: ‘Seeking revenge' over Pakistan ties"
# 5. Tokenize the text and move tensors to the correct device
inputs = tokenizer(inputText, return_tensors="pt").to(device)

# 6. Make a prediction
with torch.no_grad(): # Disable gradient calculation for faster inference
    logits = model(**inputs).logits

# 7. Get the predicted class ID (0 or 1)
predicted_class_id = logits.argmax().item()

# 8. Print the final, human-readable result
print(f"Input Text: '{inputText}'")
if predicted_class_id == 1:
    print("--> Prediction: Fake News")
else:
    print("--> Prediction: True News")

# # Inspect a batch from the training dataloader
# for batch in trainer.get_train_dataloader():
#     print(batch['labels'].dtype)
#     print(batch['input_ids'].dtype)
#     print(batch['attention_mask'].dtype)
#     break # Just inspect one batch

print("Hello")

