# -*- coding: utf-8 -*-
"""FNDFDS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bCHyCqpQ9QEB6r54AUg3SmZ2mM-bZZc
"""

!pip install transformers datasets evaluate scikit-learn

import pandas as pd

train_df = pd.read_csv('/content/multimodal_train.tsv', sep='\t')
val_df   = pd.read_csv('/content/multimodal_validate.tsv', sep='\t')
test_df  = pd.read_csv('/content/multimodal_test_public.tsv', sep='\t')

print(train_df.columns)  # check available columns

print(train_df.shape)
print(val_df.shape)
print(test_df.shape)

# Select the first 10,000 rows of each dataframe
train_df_small = train_df.iloc[:10000]
val_df_small = val_df.iloc[:2000]
test_df_small = test_df.iloc[:2000]

# Verify the new shapes
print(train_df_small.shape)
print(val_df_small.shape)
print(test_df_small.shape)

from datasets import Dataset

train_ds = Dataset.from_pandas(train_df_small[['clean_title', '2_way_label']])
val_ds   = Dataset.from_pandas(val_df_small[['clean_title', '2_way_label']])
test_ds  = Dataset.from_pandas(test_df_small[['clean_title', '2_way_label']])

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["clean_title"], truncation=True, padding="max_length", max_length=64)

train_ds = train_ds.map(tokenize, batched=True)
val_ds   = val_ds.map(tokenize, batched=True)
test_ds  = test_ds.map(tokenize, batched=True)

train_ds = train_ds.rename_column("2_way_label", "labels")
val_ds   = val_ds.rename_column("2_way_label", "labels")
test_ds  = test_ds.rename_column("2_way_label", "labels")

train_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

!pip install --upgrade transformers

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate

# Load BERT
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Metrics
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels),
        "f1": f1.compute(predictions=preds, references=labels)
    }

# Training Args
training_args = TrainingArguments(
    output_dir="./results",


    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

print(trainer.evaluate(test_ds))

model.save_pretrained("./text_model")
tokenizer.save_pretrained("./text_model")

# # Inspect a batch from the training dataloader
# for batch in trainer.get_train_dataloader():
#     print(batch['labels'].dtype)
#     print(batch['input_ids'].dtype)
#     print(batch['attention_mask'].dtype)
#     break # Just inspect one batch

print("Hello")

